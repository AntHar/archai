__include__: "../datasets/cifar10.yaml" # default dataset settings are for cifar

common:
  experiment_name: 'throwaway' # you should supply from command line
  experiment_desc: 'throwaway'
  logdir: '~/logdir'
  seed: 2.0
  tb_enable: False # if True then TensorBoard logging is enabled (may impact perf)
  tb_dir: '$expdir/tb' # path where tensorboard logs would be stored
  horovod: False
  device: 'cuda'
  checkpoint:
    filename: '$expdir/checkpoint.pth'
    freq: 10
  detect_anomaly: False # if True, PyTorch code will run 6X slower
  # TODO: workers setting

  # reddis address of Ray cluster. Use None for single node run
  # otherwise it should something like host:6379. Make sure to run on head node:
  # "ray start --head --redis-port=6379"
  redis: null
  gpus: 0 # use GPU IDs specified here (comma separated), if null then use all GPUs

  smoke_test: False
  only_eval: False
  resume: True

dataset: {} # default dataset settings comes from __include__ on the top

nas:
  eval:
    full_desc_filename: '$expdir/full_model_desc.yaml' # model desc used for building model for evaluation
    final_desc_filename: '$expdir/final_model_desc.yaml' # model desc used as template to construct cells

    # if below is specified then final_desc_filename is ignored and model is created through factory function instead.
    # this is useful for running eval for manually designed models such as resnet-50
    # keys: module, function, args. module is fully namespaced module where function exist. The function
    # returns nn.Module. If args exist, it should be dictionary and it will be passed as **kwargs to the function.
    final_model_factory: {}

    metric_filename: '$expdir/eval_train_metrics.yaml'
    model_filename: '$expdir/model.pt' # file to which trained model will be saved
    device: '_copy: common/device'
    data_parallel: False
    checkpoint:
      _copy: 'common/checkpoint'
    resume: '_copy: common/resume'
    model_desc:
      init_ch_out: 36 # num of channels for stem outpt node
      n_cells: 20 # number of cells
      n_reductions: 2 # number of reductions to be applied
      n_nodes: 4 # number of nodes in a cell
      stem_multiplier: 3 # output channels multiplier for the stem
      aux_weight: 0.4 # weight for loss from auxiliary towers in test time arch
      dataset:
        _copy: '/dataset'
      max_final_edges: 2 # max edge that can be in final arch per node
      cell_post_op: 'concate_channels'
      model_stem0_op: 'stem_conv3x3'
      model_stem1_op: 'stem_conv3x3'
      model_post_op: 'pool_adaptive_avg2d'
      params: {}
    loader:
      aug: '' # additional augmentations to use
      cutout: 16 # cutout length, use cutout augmentation when > 0
      load_train: True # load train split of dataset
      train_batch: 96
      train_workers: null # if null then gpu_count*4
      load_test: True # load test split of dataset
      test_batch: 1024
      test_workers: null # if null then gpu_count*4
      val_ratio: 0.0 #split portion for test set, 0 to 1
      val_fold: 0 #Fold number to use (0 to 4)
      cv_num: 5 # total number of folds available
      horovod: '_copy: common/horovod'
      dataset:
        _copy: '/dataset'
    trainer:
      apex: False
      aux_weight: '_copy: nas/eval/model_desc/aux_weight'
      drop_path_prob: 0.2 # probability that given edge will be dropped
      grad_clip: 5.0 # grads above this value is clipped
      l1_alphas: 0.0   # weight to be applied to sum(abs(alphas)) to loss term
      logger_freq: 1000 # after every N updates dump loss and other metrics in logger
      title: 'eval_train'
      epochs: 600
      lossfn:
        type: 'CrossEntropyLoss'
      optimizer:
        type: 'sgd'
        lr: 0.025 # init learning rate
        decay: 3.0e-4 # pytorch default is 0.0
        momentum: 0.9 # pytorch default is 0.0
        nesterov: False # pytorch default is False
        warmup: null
      lr_schedule:
        type: 'cosine'
        min_lr: 0.001 # min learning rate to se bet in eta_min param of scheduler
      validation:
        title: 'eval_test'
        logger_freq: 0
        freq: 1 # perform validation only every N epochs
        lossfn:
          type: 'CrossEntropyLoss'

  search:
    data_parallel: False
    checkpoint:
      _copy: 'common/checkpoint'
    resume: '_copy: common/resume'
    search_iters: 1
    full_desc_filename: '$expdir/full_model_desc.yaml' # arch before it was finalized
    final_desc_filename: '$expdir/final_model_desc.yaml' # final arch is saved in this file
    metrics_dir: '$expdir/models/{reductions}/{cells}/{nodes}/{search_iter}' # where metrics and model stats would be saved from each pareto iteration
    device: '_copy: common/device'
    seed_train:
      trainer:
        _copy: 'nas/eval/trainer'
        title: 'seed_train'
        epochs: 0 # number of epochs model will be trained before search
        aux_weight: 0.0
        drop_path_prob: 0.0
      loader:
        _copy: 'nas/eval/loader'
        train_batch: 128
        val_ratio: 0.1 #split portion for test set, 0 to 1
    post_train:
      trainer:
        _copy: 'nas/eval/trainer'
        title: 'post_train'
        epochs: 0 # number of epochs model will be trained after search
        aux_weight: 0.0
        drop_path_prob: 0.0
      loader:
        _copy: 'nas/eval/loader'
        train_batch: 128
        val_ratio: 0.1 #split portion for test set, 0 to 1
    pareto:
      # default parameters are set so there is exactly one search iteration
      max_cells: 8
      max_reductions: 2
      max_nodes: 4
      enabled: False
      summary_filename: '$expdir/perito.tsv' # for each iteration of macro, we fave model and perf summary
    model_desc:
      _copy: 'nas/eval/model_desc'
      init_ch_out: 16 # num of output channels for the first cell
      n_cells: 8 # number of cells
      n_reductions: 2 # number of reductions to be applied
      aux_weight: 0.0 # weight for loss from auxiliary towers in test time arch
      params: {} # additional model desc params for algos
    loader:
      aug: '' # additional augmentations to use
      cutout: 0 # cutout length, use cutout augmentation when > 0
      load_train: True # load train split of dataset
      train_batch: 64
      train_workers: null # if null then gpu_count*4
      load_test: False # load test split of dataset
      test_batch: 1024
      test_workers: null # if null then gpu_count*4
      val_ratio: 0.5 #split portion for test set, 0 to 1
      val_fold: 0 #Fold number to use (0 to 4)
      cv_num: 5 # total number of folds available
      horovod: '_copy: common/horovod'
      dataset:
        _copy: '/dataset'
    trainer:
      apex: False
      aux_weight: '_copy: nas/search/model_desc/aux_weight'
      drop_path_prob: 0.0 # probability that given edge will be dropped
      grad_clip: 5.0 # grads above this value is clipped
      logger_freq: 1000 # after every N updates dump loss and other metrics in logger
      title: 'arch_train'
      epochs: 50
      # additional vals for the derived class
      plotsdir: '' #empty string means no plots, other wise plots are generated for each epoch in this dir
      l1_alphas: 0.0   # weight to be applied to sum(abs(alphas)) to loss term
      lossfn:
        type: 'CrossEntropyLoss'
      optimizer:
        type: 'sgd'
        lr: 0.025 # init learning rate
        decay: 3.0e-4
        momentum: 0.9 # pytorch default is 0
        nesterov: False
        warmup: null
      alpha_optimizer:
        type: 'adam'
        lr: 3.0e-4
        decay: 1.0e-3
        betas: [0.5, 0.999]
      lr_schedule:
        type: 'cosine'
        min_lr: 0.001 # min learning rate, this will be used in eta_min param of scheduler
      validation:
        title: 'search_val'
        logger_freq: 0
        freq: 1 # perform validation only every N epochs
        lossfn:
          type: 'CrossEntropyLoss'


autoaug:
  num_op: 2
  num_policy: 5
  num_search: 200
  num_result_per_cv: 10 # after conducting N trials, we will chose the results of top num_result_per_cv
  loader:
    aug: '' # additional augmentations to use
    cutout: 16 # cutout length, use cutout augmentation when > 0
    epochs: 50
    load_train: True # load train split of dataset
    train_batch: 64
    train_workers: null # if null then gpu_count*4
    load_test: True # load test split of dataset
    test_batch: 1024
    test_workers: null # if null then gpu_count*4
    val_ratio: 0.4 #split portion for test set, 0 to 1
    val_fold: 0 #Fold number to use (0 to 4)
    cv_num: 5 # total number of folds available
    horovod: '_copy: common/horovod'
    dataset:
      _copy: '/dataset'
  optimizer:
    type: 'sgd'
    lr: 0.025 # init learning rate
    decay: 3.0e-4 # pytorch default is 0.0
    momentum: 0.9 # pytorch default is 0.0
    nesterov: False # pytorch default is False
    clip: 5.0 # grads above this value is clipped # TODO: Why is this also in trainer?
    warmup:
      null
      # multiplier: 2
      # epochs: 3
    #betas: [0.9, 0.999] # PyTorch default betas for Adam
  lr_schedule:
    type: 'cosine'
    min_lr: 0.0 # min learning rate, this will be used in eta_min param of scheduler
